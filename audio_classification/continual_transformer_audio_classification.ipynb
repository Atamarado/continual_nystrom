{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [GTZAN](https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification) and change the value of `GTZAN_DIR` below accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_GPUS = [7]  # which GPUs to use\n",
    "\n",
    "import os\n",
    "\n",
    "ROOT_DIR = '..'\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(gpu_number) for gpu_number in SELECTED_GPUS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown==4.2.0 librosa==0.8.0 pytorch-lightning==1.5.3 ptflops==0.6.7 continual-inference==0.15.3 tensorflow==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import urllib.request\n",
    "\n",
    "download_list = [\n",
    "    (\n",
    "        'https://drive.google.com/u/0/uc?id=1mhqXZ8CANgHyepum7N4yrjiyIg6qaMe6',\n",
    "        'vggish_audioset_weights.h5'\n",
    "    ),\n",
    "    (\n",
    "        'https://drive.google.com/u/0/uc?id=16JrWEedwaZFVZYvn1woPKCuWx85Ghzkp',\n",
    "        'vggish_audioset_weights_without_fc2.h5'\n",
    "    ),\n",
    "    (\n",
    "        'https://raw.githubusercontent.com/DTaoo/VGGish/master/mel_features.py',\n",
    "        'mel_features.py'\n",
    "    ),\n",
    "    (\n",
    "        'https://raw.githubusercontent.com/DTaoo/VGGish/master/preprocess_sound.py',\n",
    "        'preprocess_sound.py'\n",
    "    ),\n",
    "    (\n",
    "        'https://raw.githubusercontent.com/DTaoo/VGGish/master/vggish_params.py',\n",
    "        'vggish_params.py'\n",
    "    )\n",
    "]\n",
    "\n",
    "for url, file_path in download_list:\n",
    "    if not os.path.exists(file_path):\n",
    "        if 'drive.google.com' in url:\n",
    "            gdown.download(\n",
    "                url,\n",
    "                file_path,\n",
    "                quiet=False\n",
    "            )\n",
    "        else:\n",
    "            urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving this issue: https://github.com/DTaoo/VGGish/issues/11\n",
    "params_path = 'vggish_params.py'\n",
    "with open(params_path, 'rt') as read_file:\n",
    "    text = read_file.read()\n",
    "with open(params_path, 'wt') as write_file:\n",
    "    write_file.write(text.replace('496', '96').replace('4.96', '0.96'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
    "\n",
    "GPUS = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in GPUS:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "import librosa\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from audioread import NoBackendError\n",
    "from preprocess_sound import preprocess_sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_SIZE = (96, 64, 1)\n",
    "GTZAN_DIR = '/tf/gtzan_data'\n",
    "GTZAN_CSV_PATH = os.path.join(GTZAN_DIR, 'features_30_sec.csv')\n",
    "GTZAN_WAVEFORM_DIR = os.path.join(GTZAN_DIR, 'genres_original')\n",
    "GTZAN_SAMPLING_RATE = 22050\n",
    "GTZAN_LENGTH = 30  # seconds\n",
    "GTZAN_SUB_LENGTH = 1  # seconds\n",
    "GTZAN_SUB_HOP = 0.25  # seconds\n",
    "GTZAN_VGGISH_VAL_RATIO = 0.1\n",
    "GTZAN_VGGISH_TEST_RATIO = 0.1\n",
    "GTZAN_VIT_VAL_RATIO = 0.18\n",
    "GTZAN_VIT_TEST_RATIO = 0.1\n",
    "GTZAN_SPECTROGRAMS_CACHE_PATH = 'gtzan_spectrograms.pkl'\n",
    "GTZAN_FEATURES_CACHE_PATH = 'gtzan_features.pkl'\n",
    "FINE_TUNED_VGGISH_PATH = 'fine_tuned_vggish.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGGish(load_weights=True, weights='audioset',\n",
    "           input_tensor=None, input_shape=AUDIO_SIZE,\n",
    "           out_dim=128, include_top=True, pooling='avg'):\n",
    "    if weights not in {'audioset', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `audioset` '\n",
    "                         '(pre-training on audioset).')\n",
    "    if input_tensor is None:\n",
    "        aud_input = tf.keras.layers.Input(shape=input_shape, name='input_1')\n",
    "    else:\n",
    "        aud_input = input_tensor\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv1')(aud_input)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool1')(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv2')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool2')(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv3/conv3_1')(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv3/conv3_2')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool3')(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv4/conv4_1')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv4/conv4_2')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool4')(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = tf.keras.layers.Flatten(name='flatten_')(x)\n",
    "        x = tf.keras.layers.Dense(4096, activation='relu', name='vggish_fc1/fc1_1')(x)\n",
    "        x = tf.keras.layers.Dense(4096, activation='relu', name='vggish_fc1/fc1_2')(x)\n",
    "        x = tf.keras.layers.Dense(out_dim, activation='relu', name='vggish_fc2')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    model = tf.keras.models.Model(aud_input, x, name='VGGish')\n",
    "\n",
    "    if load_weights:\n",
    "        if weights == 'audioset':\n",
    "            if include_top:\n",
    "                model.load_weights('vggish_audioset_weights.h5')\n",
    "            else:\n",
    "                model.load_weights('vggish_audioset_weights_without_fc2.h5')\n",
    "        else:\n",
    "            print(\"failed to load weights\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_waveform_size(waveform):\n",
    "    correct_size = GTZAN_LENGTH * GTZAN_SAMPLING_RATE\n",
    "    if waveform.shape[0] < correct_size:\n",
    "        zero_padding = np.zeros(correct_size - waveform.shape[0])\n",
    "        waveform = np.concatenate([waveform, zero_padding])\n",
    "    elif waveform.shape[0] > correct_size:\n",
    "        waveform = waveform[:correct_size]\n",
    "    return waveform\n",
    "\n",
    "def drange(start, stop, step):\n",
    "    r = start\n",
    "    while r < stop:\n",
    "        yield r\n",
    "        r += step\n",
    "\n",
    "def get_waveforms(waveform):\n",
    "    correct_waveform_size(waveform)\n",
    "    waveforms = []\n",
    "    for i in drange(0, GTZAN_LENGTH, GTZAN_SUB_HOP):\n",
    "        start_index = int(i * GTZAN_SUB_HOP * GTZAN_SAMPLING_RATE)\n",
    "        end_index = start_index + int(GTZAN_SUB_LENGTH * GTZAN_SAMPLING_RATE)\n",
    "        sub_waveform = waveform[start_index:end_index]\n",
    "        waveforms.append(sub_waveform)\n",
    "    return waveforms\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p] \n",
    "\n",
    "def get_waveforms_and_labels():\n",
    "    csv_data = pd.read_csv(GTZAN_CSV_PATH)\n",
    "    string_labels = csv_data.label.unique()\n",
    "    csv_data['num_label'] = csv_data.apply(lambda row: np.where(string_labels == row['label'])[0][0], axis=1)\n",
    "    all_waveforms = []\n",
    "    all_labels = []\n",
    "    for index, row in csv_data.iterrows():\n",
    "        sys.stdout.write('\\rWaveform %d' % (index + 1))\n",
    "        sys.stdout.flush()\n",
    "        waveform_path = os.path.join(GTZAN_WAVEFORM_DIR, row['label'], row['filename'])\n",
    "        try:\n",
    "            waveform, _ = librosa.load(waveform_path, sr=GTZAN_SAMPLING_RATE)\n",
    "        except NoBackendError:  # one file in the dataset is known to be corrupt\n",
    "            print('Skipping corrupt file: %s' % waveform_path)\n",
    "            continue\n",
    "        waveforms = get_waveforms(waveform)\n",
    "        label = row['num_label']\n",
    "        all_waveforms.append(waveforms)\n",
    "        all_labels.append(label)\n",
    "    print()  # newline\n",
    "    all_waveforms = np.array(all_waveforms)\n",
    "    all_labels = tf.keras.utils.to_categorical(np.array(all_labels))\n",
    "    all_waveforms, all_labels = unison_shuffled_copies(all_waveforms, all_labels)\n",
    "    return all_waveforms, all_labels\n",
    "\n",
    "def get_spectrograms_and_labels():\n",
    "    if os.path.exists(GTZAN_SPECTROGRAMS_CACHE_PATH):\n",
    "        with open(GTZAN_SPECTROGRAMS_CACHE_PATH, 'rb') as gtzan_file:\n",
    "            cached_data = pickle.load(gtzan_file)\n",
    "        all_spectrograms = cached_data['all_spectrograms']\n",
    "        all_labels = cached_data['all_labels']\n",
    "    else:\n",
    "        all_waveforms, all_labels = get_waveforms_and_labels()\n",
    "        all_spectrograms = []\n",
    "        for index, waveforms in enumerate(all_waveforms):\n",
    "            sys.stdout.write('\\rSpectrogram %d/%d' % (index + 1, len(all_waveforms)))\n",
    "            sys.stdout.flush()\n",
    "            spectrograms = []\n",
    "            for waveform in waveforms:\n",
    "                spectrogram = preprocess_sound(waveform, GTZAN_SAMPLING_RATE)\n",
    "                spectrogram = np.moveaxis(spectrogram, 0, -1)\n",
    "                spectrograms.append(spectrogram)\n",
    "            all_spectrograms.append(spectrograms)\n",
    "        print()  # newline\n",
    "        with open(GTZAN_SPECTROGRAMS_CACHE_PATH, 'wb') as cache_file:\n",
    "            pickle.dump({\n",
    "                'all_spectrograms': all_spectrograms,\n",
    "                'all_labels': all_labels,\n",
    "            }, cache_file, protocol=4)\n",
    "    return all_spectrograms, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFGTZANSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, split, batch_size):\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        all_spectrograms, all_labels = get_spectrograms_and_labels()\n",
    "        all_spectrograms = np.array(all_spectrograms)\n",
    "        all_labels = np.array(all_labels)\n",
    "        reshaped_spectrograms = np.reshape(\n",
    "            all_spectrograms,\n",
    "            (\n",
    "                all_spectrograms.shape[0] * all_spectrograms.shape[1],\n",
    "                all_spectrograms.shape[2],\n",
    "                all_spectrograms.shape[3],\n",
    "                all_spectrograms.shape[4]\n",
    "            )\n",
    "        )\n",
    "        reshaped_labels = np.repeat(all_labels, all_spectrograms.shape[1], axis=0)\n",
    "        val_split = int((1 - GTZAN_VGGISH_VAL_RATIO - GTZAN_VGGISH_TEST_RATIO) * reshaped_spectrograms.shape[0])\n",
    "        test_split = int((1 - GTZAN_VGGISH_TEST_RATIO) * reshaped_spectrograms.shape[0])\n",
    "        if self.split == 'train':\n",
    "            self.spectrograms = reshaped_spectrograms[:val_split]\n",
    "            self.labels = reshaped_labels[:val_split]\n",
    "        elif self.split == 'val':\n",
    "            self.spectrograms = reshaped_spectrograms[val_split:test_split]\n",
    "            self.labels = reshaped_labels[val_split:test_split]\n",
    "        else:\n",
    "            self.spectrograms = reshaped_spectrograms[test_split:]\n",
    "            self.labels = reshaped_labels[test_split:]\n",
    "        self.random_permutation = np.random.permutation(len(self.spectrograms))\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.spectrograms) / self.batch_size)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.random_permutation = np.random.permutation(len(self.spectrograms))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.split == 'train':\n",
    "            return self.spectrograms[self.random_permutation[index * self.batch_size: (index + 1) * self.batch_size]], \\\n",
    "                   self.labels[self.random_permutation[index * self.batch_size: (index + 1) * self.batch_size]]\n",
    "        else:\n",
    "            return self.spectrograms[index * self.batch_size: (index + 1) * self.batch_size], \\\n",
    "                   self.labels[index * self.batch_size: (index + 1) * self.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_to_one_hot(softmax):\n",
    "    one_hot = np.zeros(softmax.shape)\n",
    "    for i in range(len(softmax)):\n",
    "        one_hot[i, np.argmax(softmax[i])] = 1\n",
    "    return one_hot\n",
    "\n",
    "def get_majority_voting_accuracy(model):\n",
    "    all_spectrograms, all_labels = get_spectrograms_and_labels()\n",
    "    all_spectrograms = np.array(all_spectrograms)\n",
    "    all_labels = np.array(all_labels)\n",
    "    val_split = int((1 - GTZAN_VGGISH_VAL_RATIO - GTZAN_VGGISH_TEST_RATIO) * all_spectrograms.shape[0])\n",
    "    test_split = int((1 - GTZAN_VGGISH_TEST_RATIO) * all_spectrograms.shape[0])\n",
    "    spectrograms = all_spectrograms[test_split:]\n",
    "    labels = all_labels[test_split:]\n",
    "    correct_count = 0\n",
    "    for clip_index in range(len(spectrograms)):\n",
    "        clip_spectrograms = spectrograms[clip_index]\n",
    "        clip_label = labels[clip_index]\n",
    "        clip_predictions = model(clip_spectrograms)\n",
    "        clip_majority_vote = np.argmax(np.sum(softmax_to_one_hot(clip_predictions), axis=0))\n",
    "        if clip_majority_vote == np.argmax(clip_label):\n",
    "            correct_count += 1\n",
    "    return correct_count / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_train(config):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    test_sequence = TFGTZANSequence('test', config['batch_size'])\n",
    "    if config['retrain'] or not os.path.exists(FINE_TUNED_VGGISH_PATH):\n",
    "        train_sequence = TFGTZANSequence('train', config['batch_size'])\n",
    "        val_sequence = TFGTZANSequence('val', config['batch_size'])\n",
    "        vggish = VGGish(\n",
    "            include_top=True,\n",
    "            load_weights=True,\n",
    "            input_shape=AUDIO_SIZE\n",
    "        )\n",
    "        output = vggish.get_layer('vggish_fc2').output\n",
    "        output = tf.keras.layers.Dense(\n",
    "            units=10,\n",
    "            activation='sigmoid'\n",
    "        )(output)\n",
    "        model = tf.keras.models.Model(\n",
    "            vggish.get_layer(index=0).input,\n",
    "            outputs=output\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=config['lr']),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_accuracy',\n",
    "            factor=0.6,\n",
    "            patience=2,\n",
    "            verbose=1,\n",
    "            mode='max',\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        )\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            FINE_TUNED_VGGISH_PATH,\n",
    "            monitor='val_accuracy',\n",
    "            verbose=1,\n",
    "            save_weights_only=False,\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            save_freq='epoch'\n",
    "        )\n",
    "        history = model.fit(\n",
    "            train_sequence,\n",
    "            validation_data=val_sequence,\n",
    "            epochs=config['epochs'],\n",
    "            shuffle=True,\n",
    "            callbacks=[\n",
    "                lr_reduce,\n",
    "                early_stop,\n",
    "                checkpoint\n",
    "            ],\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        model = tf.keras.models.load_model(FINE_TUNED_VGGISH_PATH)\n",
    "    test_accuracy = model.evaluate(test_sequence)[1] * 100\n",
    "    majority_accuracy = get_majority_voting_accuracy(model) * 100\n",
    "    print('Test Acc: %.2f, Majority Acc: %.2f' % (test_accuracy, majority_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_config = {\n",
    "    'batch_size': 64,\n",
    "    'epochs': 100,\n",
    "    'lr': 1e-4,\n",
    "    'retrain': False,\n",
    "}\n",
    "tf_train(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n",
    "\n",
    "def get_tf_params(model):\n",
    "    string_list = []\n",
    "    model.summary(print_fn=lambda x: string_list.append(x))\n",
    "    for string in string_list:\n",
    "        if string.startswith('Trainable params:'):\n",
    "            return int(string.split()[-1].replace(',', ''))\n",
    "    return None\n",
    "\n",
    "def get_tf_flops(model):\n",
    "    \"\"\"\n",
    "    from https://github.com/tensorflow/tensorflow/issues/32809#issuecomment-768977280\n",
    "    \"\"\"\n",
    "    concrete = tf.function(lambda inputs: model(inputs))\n",
    "    concrete_func = concrete.get_concrete_function(\n",
    "        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n",
    "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.graph_util.import_graph_def(graph_def, name='')\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n",
    "        return flops.total_float_ops / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = tf.keras.models.load_model(FINE_TUNED_VGGISH_PATH)\n",
    "head_params = get_tf_params(head_model)\n",
    "head_flops = get_tf_flops(head_model)\n",
    "print('Head: params %.2fM; FLOPS %.2fM' % (head_params / 10 ** 6, head_flops / 10 ** 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels():\n",
    "    if os.path.exists(GTZAN_FEATURES_CACHE_PATH):\n",
    "        with open(GTZAN_FEATURES_CACHE_PATH, 'rb') as gtzan_file:\n",
    "            cached_data = pickle.load(gtzan_file)\n",
    "        all_features = cached_data['all_features']\n",
    "        all_labels = cached_data['all_labels']\n",
    "    else:\n",
    "        all_spectrograms, all_labels = get_spectrograms_and_labels()\n",
    "        fine_tuned_vggish = tf.keras.models.load_model(FINE_TUNED_VGGISH_PATH)\n",
    "        model = tf.keras.models.Model(\n",
    "            fine_tuned_vggish.get_layer(index=0).input,\n",
    "            outputs=fine_tuned_vggish.get_layer('vggish_fc2').output\n",
    "        )\n",
    "        all_features = []\n",
    "        for index, spectrograms in enumerate(all_spectrograms):\n",
    "            sys.stdout.write('\\rFeature %d/%d' % (index + 1, len(all_spectrograms)))\n",
    "            sys.stdout.flush()\n",
    "            features = []\n",
    "            for spectrogram in spectrograms:\n",
    "                feature = model(np.expand_dims(spectrogram, axis=0))\n",
    "                features.append(tf.squeeze(feature))\n",
    "            all_features.append(features)\n",
    "        print()  # newline\n",
    "        with open(GTZAN_FEATURES_CACHE_PATH, 'wb') as cache_file:\n",
    "            pickle.dump({\n",
    "                'all_features': all_features,\n",
    "                'all_labels': all_labels,\n",
    "            }, cache_file, protocol=4)\n",
    "    all_features = np.array(all_features)\n",
    "    all_labels = np.array(all_labels)\n",
    "    val_split = int((1 - GTZAN_VIT_VAL_RATIO - GTZAN_VIT_TEST_RATIO) * all_features.shape[0])\n",
    "    test_split = int((1 - GTZAN_VIT_TEST_RATIO) * all_features.shape[0])\n",
    "    train_features = all_features[:val_split]\n",
    "    train_labels = all_labels[:val_split]\n",
    "    val_features = all_features[val_split:test_split]\n",
    "    val_labels = all_labels[val_split:test_split]\n",
    "    test_features = all_features[test_split:]\n",
    "    test_labels = all_labels[test_split:]\n",
    "    return (train_features, train_labels), (val_features, val_labels), (test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchGTZANDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.split = split\n",
    "        (train_features, train_labels), \\\n",
    "        (val_features, val_labels), \\\n",
    "        (test_features, test_labels) = get_features_and_labels()\n",
    "        if split == 'train':\n",
    "            self.features = train_features\n",
    "            self.labels = train_labels\n",
    "        elif split == 'val':\n",
    "            self.features = val_features\n",
    "            self.labels = val_labels\n",
    "        else:\n",
    "            self.features = test_features\n",
    "            self.labels = test_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import continual as co\n",
    "\n",
    "from continual_transformers import (\n",
    "    CircularPositionalEncoding,\n",
    "    CoReSiTransformerEncoder,\n",
    "    CoSiTransformerEncoder,\n",
    ")\n",
    "\n",
    "INPUT_DIM = 128\n",
    "SEQ_LEN = 120\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_position_embeddings, embedding_dim, seq_length):\n",
    "        super(LearnedPositionalEncoding, self).__init__()\n",
    "        self.pe = nn.Embedding(max_position_embeddings, embedding_dim)\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(max_position_embeddings).expand((1, -1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, position_ids=None):\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, : self.seq_length]\n",
    "\n",
    "        position_embeddings = self.pe(position_ids)\n",
    "        position_embeddings = torch.permute(position_embeddings, (0, 2, 1))\n",
    "        return x + position_embeddings\n",
    "\n",
    "def CoTransformerModel(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    heads,\n",
    "    mlp_dim,\n",
    "    dropout_rate=0.1,\n",
    "    sequence_len=64,\n",
    "):\n",
    "    assert depth in {1, 2}\n",
    "\n",
    "    if depth == 1:\n",
    "        return CoSiTransformerEncoder(\n",
    "            sequence_len=sequence_len,\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=heads,\n",
    "            dropout=dropout_rate,\n",
    "            in_proj_bias=False,\n",
    "            query_index=-1,\n",
    "            ff_hidden_dim=mlp_dim,\n",
    "            ff_activation=nn.GELU(),\n",
    "            device=None,\n",
    "            dtype=None,\n",
    "        )\n",
    "\n",
    "    # depth == 2\n",
    "    return CoReSiTransformerEncoder(\n",
    "        sequence_len=sequence_len,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=heads,\n",
    "        dropout=dropout_rate,\n",
    "        in_proj_bias=False,\n",
    "        query_index=-1,\n",
    "        ff_hidden_dim=mlp_dim,\n",
    "        ff_activation=nn.GELU(),\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    )\n",
    "\n",
    "def CoVisionTransformer(\n",
    "    sequence_len,\n",
    "    input_dim,\n",
    "    embedding_dim,\n",
    "    attn_ff_hidden_dim,\n",
    "    out_dim,\n",
    "    num_heads,\n",
    "    num_layers,\n",
    "    dropout_rate=0.1,\n",
    "):\n",
    "\n",
    "    assert embedding_dim % num_heads == 0\n",
    "\n",
    "    linear_encoding = co.Linear(input_dim, embedding_dim, channel_dim=1)\n",
    "    position_encoding = CircularPositionalEncoding(\n",
    "        embedding_dim,\n",
    "        int(embedding_dim * 1.0),  # Change num pos enc to cycle between\n",
    "        forward_update_index_steps=1,\n",
    "    )\n",
    "\n",
    "    pe_dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    encoder = CoTransformerModel(\n",
    "        embedding_dim,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        attn_ff_hidden_dim,\n",
    "        dropout_rate,\n",
    "        sequence_len,\n",
    "    )\n",
    "    pre_head_ln = co.Lambda(nn.LayerNorm(embedding_dim), takes_time=False)\n",
    "    mlp_head = co.Linear(embedding_dim, out_dim, channel_dim=1)\n",
    "\n",
    "    return co.Sequential(\n",
    "        linear_encoding,\n",
    "        position_encoding,\n",
    "        pe_dropout,\n",
    "        encoder,\n",
    "        pre_head_ln,\n",
    "        mlp_head,\n",
    "    )\n",
    "\n",
    "def NonCoVisionTransformer(\n",
    "    sequence_len,\n",
    "    input_dim,\n",
    "    embedding_dim,\n",
    "    attn_ff_hidden_dim,\n",
    "    out_dim,\n",
    "    num_heads,\n",
    "    num_layers,\n",
    "    dropout_rate=0.1,\n",
    "):\n",
    "\n",
    "    assert embedding_dim % num_heads == 0\n",
    "\n",
    "    linear_encoding = co.Linear(input_dim, embedding_dim, channel_dim=1)\n",
    "    position_encoding = LearnedPositionalEncoding(\n",
    "        embedding_dim,\n",
    "        embedding_dim,\n",
    "        sequence_len\n",
    "    )\n",
    "\n",
    "    pe_dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    encoder = CoTransformerModel(\n",
    "        embedding_dim,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        attn_ff_hidden_dim,\n",
    "        dropout_rate,\n",
    "        sequence_len,\n",
    "    )\n",
    "    pre_head_ln = co.Lambda(nn.LayerNorm(embedding_dim), takes_time=False)\n",
    "    mlp_head = co.Linear(embedding_dim, out_dim, channel_dim=1)\n",
    "\n",
    "    return nn.Sequential(\n",
    "        linear_encoding,\n",
    "        position_encoding,\n",
    "        pe_dropout,\n",
    "        encoder,\n",
    "        pre_head_ln,\n",
    "        mlp_head,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader):\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader):\n",
    "            features, labels = data\n",
    "            features = torch.permute(features, (0, 2, 1))\n",
    "            features = features.cuda()\n",
    "            labels = labels.cuda()\n",
    "            predicted_labels = model(features)\n",
    "            predicted_labels = torch.squeeze(predicted_labels, dim=-1)\n",
    "            correct_count +=  torch.sum(torch.argmax(predicted_labels, dim=1) == torch.argmax(labels, dim=1))\n",
    "            total_count += len(labels)\n",
    "    accuracy = correct_count / total_count * 100  # percent\n",
    "    return accuracy\n",
    "\n",
    "def get_model_path(config):\n",
    "    return '%s_%d_layers_%s.pth' % (\n",
    "        'continual' if config['continual'] else 'non_continual',\n",
    "        config['num_layers'],\n",
    "        config['version']\n",
    "    )\n",
    "\n",
    "def torch_train(config):\n",
    "    train_dataset = TorchGTZANDataset('train')\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['batch_size']\n",
    "    )\n",
    "    val_dataset = TorchGTZANDataset('val')\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['batch_size']\n",
    "    )\n",
    "    test_dataset = TorchGTZANDataset('test')\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['batch_size']\n",
    "    )\n",
    "\n",
    "    # create and load mode\n",
    "    if config['continual']:\n",
    "        model_class = CoVisionTransformer\n",
    "    else:\n",
    "        model_class = NonCoVisionTransformer\n",
    "    model = model_class(\n",
    "        sequence_len=SEQ_LEN,\n",
    "        input_dim=INPUT_DIM,\n",
    "        embedding_dim=192,\n",
    "        attn_ff_hidden_dim=192,\n",
    "        out_dim=10,\n",
    "        num_heads=16,\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout_rate=0.1,\n",
    "    )\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.cuda()\n",
    "\n",
    "    # optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # training loop\n",
    "    best_val_accuracy = 0.0\n",
    "    for epoch in range(config['epochs']):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # load data\n",
    "            features, labels = data\n",
    "            features = torch.permute(features, (0, 2, 1))\n",
    "            features = features.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # train the model\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predicted_labels = model(features)\n",
    "            predicted_labels = torch.squeeze(predicted_labels, dim=-1)\n",
    "            loss = criterion(predicted_labels, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training metrics\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_accuracy = calculate_accuracy(model, train_loader)\n",
    "        val_accuracy = calculate_accuracy(model, val_loader)\n",
    "        improved = False\n",
    "        if val_accuracy >= best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            improved = True\n",
    "            torch.save(model.state_dict(), get_model_path(config))\n",
    "\n",
    "        print('Epoch: %d/%d; Loss: %.2e; Train Acc: %.2f; Val Acc: %.2f%s' % (\n",
    "            epoch + 1,\n",
    "            config['epochs'],\n",
    "            running_loss,\n",
    "            train_accuracy,\n",
    "            val_accuracy,\n",
    "            '; saved' if improved else ''\n",
    "        ))\n",
    "    \n",
    "    test_accuracy = calculate_accuracy(model, test_loader)\n",
    "\n",
    "    return model, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "def get_flops_and_params(model, config):\n",
    "    if config['continual']:\n",
    "        warm_up_input = torch.randn(1, INPUT_DIM, SEQ_LEN)\n",
    "        model.to('cpu')\n",
    "        assert next(model.parameters()).is_cuda == warm_up_input.is_cuda\n",
    "        model.forward_steps(warm_up_input)  # Warm up model\n",
    "        model.call_mode = \"forward_step\"\n",
    "        flops, params = get_model_complexity_info(\n",
    "            model, (INPUT_DIM,), as_strings=False, print_per_layer_stat=False\n",
    "        )\n",
    "    else:\n",
    "        flops, params = get_model_complexity_info(\n",
    "            model, (INPUT_DIM, SEQ_LEN), as_strings=False, print_per_layer_stat=False\n",
    "        )\n",
    "    return flops, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_config = {\n",
    "    'batch_size': 32,\n",
    "    'lr': 1e-5,\n",
    "    'weight_decay': 1e-4,\n",
    "    'epochs': 50,\n",
    "    'version': 'v5',\n",
    "    'num_layers': 2,\n",
    "    'continual': False,\n",
    "}\n",
    "non_continual_model, test_accuracy = torch_train(torch_config)\n",
    "flops, params = get_flops_and_params(non_continual_model, torch_config)\n",
    "print(test_accuracy, flops, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_config = {\n",
    "    'batch_size': 32,\n",
    "    'lr': 1e-5,\n",
    "    'weight_decay': 1e-4,\n",
    "    'epochs': 50,\n",
    "    'version': 'v5',\n",
    "    'num_layers': 1,\n",
    "    'continual': True,\n",
    "}\n",
    "continual_model, test_accuracy = torch_train(torch_config)\n",
    "flops, params = get_flops_and_params(continual_model, torch_config)\n",
    "print(test_accuracy, flops, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
